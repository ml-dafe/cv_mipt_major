{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Семинар №3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-22T12:37:48.235828Z",
     "start_time": "2021-02-22T12:37:48.088067Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 \n",
    "import skimage.exposure\n",
    "import skimage.io\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-22T12:37:48.355381Z",
     "start_time": "2021-02-22T12:37:48.074Z"
    }
   },
   "outputs": [],
   "source": [
    "# вспомогательная функция\n",
    "def plot_transform_result(src_image, transform_image, is_gray=False):\n",
    "    \"\"\"\n",
    "    Отрисовать с помощью plt исходное изображение и его преобразование.\n",
    "    \n",
    "    :param src_image: np.ndarray: исходное изображение\n",
    "    :param transform_image: np.ndarray: преобразованное изображение\n",
    "    :param is_gray: bool: флаг для отображения ЧБ изображений\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(6.4 * 2, 4.8 * 2), constrained_layout=True)\n",
    "\n",
    "    cmap = 'gray' if is_gray else None\n",
    "    \n",
    "    plt.subplot(121)\n",
    "    plt.imshow(src_image, cmap=cmap)\n",
    "    plt.axis('off'), plt.title('Исходное изображение')\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    plt.imshow(transform_image, cmap=cmap)\n",
    "    plt.axis('off'), plt.title('Результат преобразования')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Гистограммы \n",
    "\n",
    "Так что такое гистограмма? Вы можете рассматривать гистограмму как график, который дает вам общее представление о распределении интенсивности изображения. Это график со значениями пикселей (от $0$ до $255$, не всегда) по оси $X$ и соответствующим количеством пикселей в изображении по оси $Y$.\n",
    "\n",
    "Это просто еще один способ понять образ. Глядя на гистограмму изображения, вы получаете представление о контрасте, яркости, распределении интенсивности и т.д. этого изображения. Почти все инструменты обработки изображений сегодня предоставляют функции на гистограмме.\n",
    "\n",
    "<img src=\"https://i.ibb.co/cJxrWhx/histogram_sample.jpg\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "Вы можете увидеть изображение и его гистограмму. Помните, эта гистограмма нарисована для изображения в оттенках серого, а не для цветного изображения. Левая область гистограммы показывает количество темных пикселей на изображении, а правая область показывает количество ярких пикселей. На гистограмме видно, что темная область больше, чем яркая, а количество полутонов (значения пикселей в среднем диапазоне, скажем, около $127$) намного меньше, чем остального."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Гистограммы** $-$ это данные, организованные в набор предопределенных столбцов.\n",
    "Когда мы говорим данные, не ограничиваем их значениями интенсивности (как мы видели в предыдущем уроке). Собранные данные могут быть любой функцией, которую вы найдете полезной для описания изображения.\n",
    "\n",
    "Посмотрим на пример. Представьте, что матрица содержит информацию об изображении (то есть интенсивность в диапазоне $0-255$):\n",
    "\n",
    "<img src=\"img/Histogram_Calculation_Theory_Hist.jpg\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "Что произойдет, если мы захотим считать эти данные организованным способом? Так как мы знаем, что диапазон информационных значений, для этого случая равен [0, 256], можем сегментировать наш диапазон по частям (называемым ячейками), например:\n",
    "\n",
    "$$\n",
    "{\\begin{array}{l}\n",
    "[0, 255] = { [0, 15] \\cup [16, 31] \\cup ....\\cup [240,255] } \\\\\n",
    "range = { bin_{1} \\cup bin_{2} \\cup ....\\cup bin_{n = 15} }\n",
    "\\end{array}}\n",
    "$$\n",
    "\n",
    "и можем вести подсчет количества пикселей, попадающих в диапазон каждого $bin_{i}$. Применяя это к примеру выше, мы получаем изображение ниже (ось x представляет ячейки, а ось y – количество пикселей в каждом из них).\n",
    "\n",
    "<img src=\"https://i.ibb.co/Wss2Fqh/Histogram_Calculation_Theory_Hist1.jpg\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "Это был простой пример того, как работает гистограмма и почему она полезна. Гистограмма может вести учет не только интенсивности цвета, но и любых характеристик изображения, которые мы хотим измерить (то есть градиенты, направления и т.д.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "Выделим части гистограммы:\n",
    "\n",
    "* __dims__: количество параметров, для которых вы хотите собрать данные. В нашем примере dims = $1$, потому что мы рассчитываем только значения интенсивности каждого пикселя (в полутоновом изображении)\n",
    "* __bins__: это количество подразделений в каждом тусклом свете. В нашем примере bins = $16$\n",
    "* __range__: пределы измеряемых значений. В этом случае: range = $[0,255]$\n",
    "\n",
    "Что если вы хотите сосчитать две особенности? В этом случае ваша результирующая гистограмма будет трехмерным графиком (в котором $x$ и $y$ будут $bin_{x}$ и $bin_{y}$ для каждого объекта, а $z$ будет количеством отсчетов для каждой комбинации $(bin_ {x}, bin_ {y})$. То же самое относится и к дополнительным функциям (конечно, это становится сложнее)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Гистограммы в OpenCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Воспользуемся функцией ```cv2.calcHist()```, чтобы найти гистограмму. Ознакомимся с функцией и ее параметрами:\n",
    "\n",
    "```cv2.calcHist (image, channel, mask, histSize, range , hist, accumulate)```\n",
    "\n",
    "* **images** $-$ это исходное изображение типа $uint8$ или $float32$. его следует указывать в квадратных скобках, т.е. **[img]**.\n",
    "* **channel** $-$ также указывается в квадратных скобках. Это индекс канала, для которого мы рассчитываем гистограмму. Например, если входное изображение представляет собой изображение в градациях серого, его значение равно $[0]$. Для цветного изображения вы можете передать $[0]$, $[1]$ или $[2]$, чтобы вычислить гистограмму синего, зеленого или красного канала соответственно.\n",
    "* **mask** $-$ маска изображения. Чтобы найти гистограмму полного изображения, она задается как **None**. Но если вы хотите найти гистограмму определенной области изображения, вы должны создать для нее изображение маски и указать ее как маску.\n",
    "* **histSize** $-$ это представляет наш счетчик __bin__. Нужно указывать в квадратных скобках. Для полной шкалы мы передаем $[256]$.\n",
    "* **range** $-$ это наш **range**. Обычно это $[0,256]$.\n",
    "* **hist** $-$ выходная гистограмма, представляющая собой плотный или разреженный размерный массив.\n",
    "* **accumulate** $-$ накопительный флаг. Если он установлен, гистограмма не очищается в начале при выделении. Эта функция позволяет вычислять одну гистограмму из нескольких наборов массивов или своевременно обновлять гистограмму."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-22T12:41:44.730540Z",
     "start_time": "2021-02-22T12:41:44.198369Z"
    },
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "image = cv2.imread('img/forest.jpg')\n",
    "h, w, _ = image.shape\n",
    "image = cv2.resize(image.copy(), (w // 2, h // 2))\n",
    "image = cv2.cvtColor(image.copy(), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# grayscale\n",
    "gray = cv2.cvtColor(image.copy(), cv2.COLOR_RGB2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.shape, h, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# получим гистограмму\n",
    "# ravel() - разворачивает массив в список\n",
    "hist = cv2.calcHist([gray.ravel()], [0], None, [256], [0, 256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6.4 * 1.2, 4.8 * 2), constrained_layout=True)\n",
    "\n",
    "plt.subplot(211)\n",
    "plt.imshow(gray, cmap='gray'),\n",
    "plt.axis('off'), plt.title('Исходное изображение')\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.plot(hist)\n",
    "plt.title('Гистограмма по освещенности')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Гистограммы в  Numpy\n",
    "\n",
    "```Numpy``` также предоставляет вам функцию ```np.histogram()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-22T12:42:57.260488Z",
     "start_time": "2021-02-22T12:42:56.922150Z"
    }
   },
   "outputs": [],
   "source": [
    "hist_np, bins = np.histogram(gray.ravel(), 256, [0,256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6.4 * 1.2, 4.8 * 2), constrained_layout=True)\n",
    "\n",
    "plt.subplot(211)\n",
    "plt.plot(hist)\n",
    "plt.title('Гистограмма по освещенности (cv2)')\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.plot(hist_np)\n",
    "plt.title('Гистограмма по освещенности (numpy)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Гистограмма аналогичная, как мы рассчитывали ранее. Но ячейки будут иметь $257$ элементов, потому что **Numpy** рассчитывает ячейки как $0-0.99$, $1-1.99$, $2-2.99$ и т.д. Таким образом, конечный диапазон будет $255-255.99$. Чтобы представить это, они также добавляют $256$ в конце bins. Но нам не нужно это $256$. До $255$ достаточно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-22T12:43:17.897593Z",
     "start_time": "2021-02-22T12:43:17.454272Z"
    }
   },
   "outputs": [],
   "source": [
    "# Визуализируем сразу 3 канала цвета\n",
    "\n",
    "plt.figure(figsize=(6.4 * 1.2, 4.8 * 2), constrained_layout=True)\n",
    "\n",
    "plt.subplot(211)\n",
    "plt.imshow(image)\n",
    "plt.title('Исходное изображение'), plt.axis('off')\n",
    "\n",
    "color = ('r', 'g', 'b')\n",
    "plt.subplot(212)\n",
    "\n",
    "for i, col in enumerate(color):\n",
    "    hist, bins = np.histogram(image[..., i].ravel(), 256, [0, 256])\n",
    "    norm_const = image.shape[0] * image.shape[1]    \n",
    "    plt.plot(hist / norm_const, color=col, label=f'{col} channel')\n",
    "    \n",
    "plt.title('Гистограмма по освещенности')\n",
    "plt.legend()    \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Применение маски\n",
    "\n",
    "Мы использовали ```cv2.calcHist()```, чтобы найти гистограмму полного изображения. Что если вы хотите найти гистограммы некоторых областей изображения? Просто создайте изображение маски белым цветом на области, где вы хотите найти гистограмму, и черным в противном случае. Затем передайте это как маску."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-22T11:47:55.754168Z",
     "start_time": "2021-02-22T11:47:55.645131Z"
    }
   },
   "outputs": [],
   "source": [
    "# grayscale\n",
    "gray = cv2.cvtColor(image.copy(), cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "# создадим маску\n",
    "mask = np.zeros(image.shape[:2], np.uint8)\n",
    "mask[50:, 600:850] = 1\n",
    "\n",
    "plt.imshow(mask, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_img = cv2.bitwise_and(gray, gray, mask=mask)\n",
    "\n",
    "plot_transform_result(gray, masked_img, is_gray=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тут мы воспользовались распространенным приемом с *\"маской\"*, т.е. использовали массив из нулей, на которой наложили интересующую картнику. \n",
    "\n",
    "Для арифметических операция над матрицами в OpenCV есть несклько базовых функций. Рассмотрим несколько из них:\n",
    "\n",
    "* __cv2.absdiff(src1, src2[,dst])__\n",
    "\n",
    "    абсолютная разница между двумя массивами: $${\\texttt{dst}(I)=|\\texttt{src1}(I)-\\texttt{src2}(I)|}$$\n",
    "\n",
    "* __cv2.add(src1, src2[, dst[, mask[, dtype]]])__\n",
    "\n",
    "    Функция add рассчитывае сумму двух массивов: $${\\texttt{dst}(I)=\\texttt{src1}(I)+\\texttt{src2}(I) \\quad\\texttt{if mask}(I)\\ne0}$$\n",
    "\n",
    "* __cv2.bitwise_and(src1, src2[, dst[, mask]])__\n",
    "\n",
    "    Функция рассчитывает побитовое логическое соединение для каждого элемента массива: $${\\texttt{dst} (I)=\\texttt{src1}(I)\\wedge\\texttt{src2}(I)\\quad\\texttt{if mask}(I)\\ne0}$$\n",
    "\n",
    "* __cv2.bitwise_not(src[, dst[, mask]])__\n",
    "\n",
    "    Функция рассчитывает побитовую инверсию входного массива для каждого элемента: $${\\texttt{dst}(I)=\\neg{\\texttt{src}(I)}}$$\n",
    "\n",
    "* __cv2.bitwise_or(src1, src2[, dst[, mask]])__\n",
    "\n",
    "    Функция вычисляет побитовую логическую дизъюнкцию для каждого элемента для: $${\\texttt{dst}(I)=\\texttt{src1}(I)\\vee\\texttt{src2}(I)\\quad\\texttt{if mask}(I)\\ne0}$$\n",
    "\n",
    "* __cv2.bitwise_xor(src1, src2[, dst[, mask]])__\n",
    "\n",
    "    Функция вычисляет побитовую логическую операцию «исключая или» для каждого элемента массива: $${\\texttt{dst}(I)=\\texttt{src1}(I)\\oplus\\texttt{src2}(I)\\quad\\texttt{if mask}(I)\\ne0}$$\n",
    "\n",
    "Остальные можно найти в [документации](https://docs.opencv.org/3.4.2/d2/de8/group__core__array.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## посчитаем гистограмму для всего изображения и маски\n",
    "hist_full = cv2.calcHist([gray], [0], None, [256], [0,256])\n",
    "hist_mask = cv2.calcHist([gray], [0], mask, [256], [0,256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-22T11:47:56.145254Z",
     "start_time": "2021-02-22T11:47:55.756172Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6.4 * 1.8, 4.8 * 1.5), constrained_layout=True)\n",
    "\n",
    "plt.subplot(221)\n",
    "plt.title('Исходное изображение')\n",
    "plt.imshow(gray, cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.title('Маска')\n",
    "plt.imshow(masked_img, cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.title('Освещенность изображения')\n",
    "plt.plot(hist_full)\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.title('Освещенность маски')\n",
    "plt.plot(hist_mask)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь посмотрим для всех каналов цвета интесивность на маске."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_img = cv2.bitwise_and(image, image, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-22T11:47:56.521327Z",
     "start_time": "2021-02-22T11:47:56.147253Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6.4 * 1.8, 4.8 * 1.5), constrained_layout=True)\n",
    "\n",
    "plt.subplot(221)\n",
    "plt.title('Исходное изображение')\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.title('Маска')\n",
    "plt.imshow(masked_img)\n",
    "plt.axis('off')\n",
    "\n",
    "color = ('r', 'g', 'b')\n",
    "for i,col in enumerate(color):\n",
    "    plt.subplot(223)\n",
    "    hist_full = cv2.calcHist([image], [i], None, [256], [0,256])\n",
    "    plt.plot(hist_full, color=col, label=f'{col} channel')\n",
    "    \n",
    "    plt.subplot(224)\n",
    "    hist_mask = cv2.calcHist([image], [i], mask, [256], [0,256])\n",
    "    plt.plot(hist_mask, color=col, label=f'{col} channel')\n",
    "\n",
    "\n",
    "plt.subplot(223), plt.title('Освещенность изображения'), plt.legend()\n",
    "plt.subplot(224), plt.title('Освещенность маски'), plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выравнивание гистограм"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenCV имеет функцию для этого, ```cv2.equalizeHist()```. Его вход $-$ это изображение в градациях серого, а выход – изображение, выровненное по гистограмме. Однако, реализация в ```cv2``` выполняется медленнее, чем ```Numpy```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выравнивание гистограмм с помощью skimage\n",
    "\n",
    "Этот пример улучшает изображение с низким контрастом, используя метод, называемый выравниванием гистограммы, который «распределяет наиболее частые значения интенсивности». Выровненное изображение имеет примерно линейную кумулятивную функцию распределения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-22T11:47:57.936937Z",
     "start_time": "2021-02-22T11:47:57.602937Z"
    }
   },
   "outputs": [],
   "source": [
    "# Equalization\n",
    "img_eq = skimage.exposure.equalize_hist(image)\n",
    "img_eq = (img_eq * 255).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_transform_result(image, img_eq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6.4 * 2, 4.8 * 1), constrained_layout=True)\n",
    "\n",
    "color = ('r', 'g', 'b')\n",
    "for i,col in enumerate(color):\n",
    "    plt.subplot(121)\n",
    "    hist_full = cv2.calcHist([image], [i], None, [256], [0,256])\n",
    "    plt.plot(hist_full, color=col, label=f'{col} channel')\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    hist_full = cv2.calcHist([img_eq], [i], None, [256], [0,256])\n",
    "    plt.plot(hist_full, color=col, label=f'{col} channel')\n",
    "\n",
    "\n",
    "plt.subplot(121), plt.title('Освещенность исходного изображения'), plt.legend()\n",
    "plt.subplot(122), plt.title('Освещенность преобразованного изображения'), plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-22T13:11:40.092707Z",
     "start_time": "2021-02-22T13:11:39.026338Z"
    }
   },
   "outputs": [],
   "source": [
    "# Adaptive Equalization\n",
    "img_adapteq = skimage.exposure.equalize_adapthist(image, kernel_size=(200,200), clip_limit=0.03)\n",
    "img_adapteq = (img_adapteq * 255).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_transform_result(image, img_adapteq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-22T13:11:41.146589Z",
     "start_time": "2021-02-22T13:11:40.720886Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6.4 * 2, 4.8 * 1), constrained_layout=False)\n",
    "\n",
    "color = ('r', 'g', 'b')\n",
    "for i,col in enumerate(color):\n",
    "    plt.subplot(121)\n",
    "    hist_full = cv2.calcHist([image], [i], None, [256], [0,256])\n",
    "    plt.plot(hist_full, color=col, label=f'{col} channel')\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    hist_full = cv2.calcHist([img_adapteq], [i], None, [256], [0,256])\n",
    "    plt.plot(hist_full, color=col, label=f'{col} channel')\n",
    "\n",
    "\n",
    "plt.subplot(121), plt.title('Освещенность исходного изображения'), plt.legend()\n",
    "plt.subplot(122), plt.title('Освещенность преобразованного изображения'), plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Сопоставление гистограмм\n",
    "\n",
    "Этот пример демонстрирует функцию сопоставления гистограмм. Он манипулирует пикселями входного изображения, чтобы его гистограмма соответствовала гистограмме эталонного изображения. Если изображения имеют несколько каналов, сопоставление выполняется независимо для каждого канала, пока количество каналов равно во входном изображении и эталонном.\n",
    "\n",
    "Сопоставление гистограммы можно использовать в качестве упрощенной нормализации для обработки изображений, например сопоставления признаков, особенно в обстоятельствах, когда изображения были взяты из разных источников или в разных условиях (например, при освещении)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = skimage.io.imread('img/forest.jpg')\n",
    "reference = skimage.io.imread('img/forest_ex.jpg')\n",
    "\n",
    "match = skimage.exposure.match_histograms(image, reference, multichannel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-22T11:48:00.184102Z",
     "start_time": "2021-02-22T11:47:59.038739Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6.4 * 3, 4.8 * 1), constrained_layout=True)\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.title('image изображение')\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.title('База для преобразования')\n",
    "plt.imshow(reference)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.title('Результат преобразования')\n",
    "plt.imshow(match)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-22T11:48:01.270681Z",
     "start_time": "2021-02-22T11:48:00.185076Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(6.4 * 3, 4.8 * 2), constrained_layout=True)\n",
    "\n",
    "for i, img in enumerate((image, reference, match)):\n",
    "    for c, c_color in enumerate(('red', 'green', 'blue')):\n",
    "        img_hist, bins = skimage.exposure.histogram(img[..., c], source_range='dtype')\n",
    "        axes[c, i].plot(bins, img_hist / img_hist.max(), color=c_color)\n",
    "        \n",
    "        img_cdf, bins = skimage.exposure.cumulative_distribution(img[..., c])\n",
    "        axes[c, i].plot(bins, img_cdf, color='black')\n",
    "        axes[c, 0].set_ylabel(c_color)\n",
    "\n",
    "axes[0, 0].set_title('Source')\n",
    "axes[0, 1].set_title('Reference')\n",
    "axes[0, 2].set_title('Matched')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сравнение гистограмм \n",
    "\n",
    "Пользы от гистограмм не будет, если их не сравнивать с эталоннами. Предположим, есть эталонное изображение $-$ какой-то объект заданных размеров. Также есть множество неизвестных изображений, на которых нужно найти эталонное изображение. Для этого нужно перебирать участки изображений, сравнивая содержимое с эталоном. Можно сравнивать каждую точку из участка, но это будет медленно. Гораздо быстрее по ресурсам $-$ это сравнить гистограммы яркости. Для сравнения гистограмм в OpenCV предусмотрена функция ```cv2.CompareHist()```.\n",
    "\n",
    "Мы рассмотрим только несколько метрик для сравнения: \n",
    "\n",
    "1. __Correlation__ (```method=cv2.HISTCMP_CORREL```)\n",
    "\n",
    "$d(H_1,H_2) =  \\frac{\\sum_I (H_1(I) - \\bar{H_1}) (H_2(I) - \\bar{H_2})}{\\sqrt{\\sum_I(H_1(I) - \\bar{H_1})^2 \\sum_I(H_2(I) - \\bar{H_2})^2}}$, где $\\bar{H_k} =  \\frac{1}{N} \\sum _J H_k(J)$\n",
    "\n",
    "2. __Chi-Square__ (```method=cv2.HISTCMP_CHISQR```)\n",
    "\n",
    "$d(H_1,H_2) =  \\sum _I  \\frac{\\left(H_1(I)-H_2(I)\\right)^2}{H_1(I)}$\n",
    "\n",
    "3. __Intersection__ (```method=cv2.HISTCMP_INTERSECT```)\n",
    "\n",
    "$d(H_1,H_2) =  1 - \\sum _I  \\min (H_1(I), H_2(I))$\n",
    "\n",
    "4. __Bhattacharyya distance__ (```method=cv2.HISTCMP_BHATTACHARYYA```).\n",
    "\n",
    "$d(H_1,H_2) =  \\sqrt{1 - \\frac{1}{\\sqrt{\\bar{H_1} \\bar{H_2} N^2}} \\sum_I \\sqrt{H_1(I) \\cdot H_2(I)}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-22T11:48:01.326531Z",
     "start_time": "2021-02-22T11:48:01.271680Z"
    }
   },
   "outputs": [],
   "source": [
    "carpet = cv2.imread('img/carpet_ex.jpg')\n",
    "carpet_gray = cv2.cvtColor(carpet, cv2.COLOR_BGR2GRAY)\n",
    "forest = cv2.imread('img/forest_ex.jpg')\n",
    "forest_gray = cv2.cvtColor(forest, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "hist_carpet = cv2.calcHist([carpet_gray.ravel()], [0], None, [256], [0,256])\n",
    "hist_forest = cv2.calcHist([forest_gray.ravel()], [0], None, [256], [0,256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_carpet /= hist_carpet.max()\n",
    "hist_forest /= hist_forest.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-22T11:48:01.679531Z",
     "start_time": "2021-02-22T11:48:01.328488Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6.4 * 2, 4.8), constrained_layout=True)\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title('Первое изображение')\n",
    "plt.imshow(carpet_gray, cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title('Второе изображение')\n",
    "plt.imshow(forest_gray, cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-22T11:48:01.805535Z",
     "start_time": "2021-02-22T11:48:01.680487Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# возьмем выравненные гистограммы освещенности и сравним через корреляцию\n",
    "\n",
    "cor_1 = cv2.compareHist(hist_carpet, hist_forest, cv2.HISTCMP_CORREL)\n",
    "\n",
    "cor_2 = cv2.compareHist(hist_carpet, hist_forest, cv2.HISTCMP_INTERSECT) / np.sum(hist_carpet)\n",
    "\n",
    "cor_3 = cv2.compareHist(hist_carpet, hist_forest, cv2.HISTCMP_BHATTACHARYYA)\n",
    "\n",
    "\n",
    "print(f'Correlation: {cor_1:.3f}\\n'\n",
    "      f'Intersection: {cor_2:.3f}\\n'\n",
    "      f'Bhattacharyya: {cor_3:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6.4 * 2, 4.8), constrained_layout=True)\n",
    "\n",
    "plt.plot(hist_carpet, color='r')\n",
    "plt.plot(hist_forest, color='gray')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Другой пример\n",
    "\n",
    "Рассмотрим изображения, которая семантически больше похожи друг на друга."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "another_forest = cv2.imread('img/forest.jpg')\n",
    "another_forest = cv2.cvtColor(another_forest, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "forest = cv2.imread('img/forest_ex.jpg')\n",
    "forest = cv2.cvtColor(forest, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6.4 * 2, 4.8), constrained_layout=True)\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title('Первое изображение')\n",
    "plt.imshow(another_forest, cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title('Второе изображение')\n",
    "plt.imshow(forest, cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим один канал и части изображений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "another_forest_g = another_forest[100:, 900:, 1]  # выделим один канал\n",
    "forest_g = forest[:300, :, 1]  # выделим один канал\n",
    "\n",
    "hist_another_forest = cv2.calcHist([another_forest_g.ravel()], [0], None, [256], [0,256])\n",
    "hist_forest = cv2.calcHist([forest_g.ravel()], [0], None, [256], [0,256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# нормировка гистограмм\n",
    "hist_another_forest /= hist_another_forest.max()\n",
    "hist_forest /= hist_forest.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6.4 * 2, 4.8), constrained_layout=True)\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title('Первое изображение')\n",
    "plt.imshow(another_forest_g, cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title('Второе изображение')\n",
    "plt.imshow(forest_g, cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# возьмем выравненные гистограммы освещенности и сравним через корреляцию\n",
    "\n",
    "cor_1 = cv2.compareHist(hist_another_forest, hist_forest, cv2.HISTCMP_CORREL)\n",
    "\n",
    "cor_2 = cv2.compareHist(hist_another_forest, hist_forest, cv2.HISTCMP_INTERSECT) / np.sum(hist_another_forest)\n",
    "\n",
    "cor_3 = cv2.compareHist(hist_another_forest, hist_forest, cv2.HISTCMP_BHATTACHARYYA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Correlation: {cor_1:.3f}\\n'\n",
    "      f'Intersection: {cor_2:.3f}\\n'\n",
    "      f'Bhattacharyya: {cor_3:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-22T11:48:02.456712Z",
     "start_time": "2021-02-22T11:48:01.806490Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6.4 * 2, 4.8), constrained_layout=True)\n",
    "\n",
    "plt.plot(hist_another_forest, color='r')\n",
    "plt.plot(hist_forest, color='gray')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заметьте, что в качестве примера были выбраны близкие гистограммы, по сути одна, но с разными операциями выравнивания. Вариативность применения коэффициента корреляции зависит от вашей фантазии и возможности интерпретировать результат.\n",
    "\n",
    "**Помните**, что корреляция показывает только **линейную** зависимость двух случайных величин. Порой связь может быть гораздо сложнее.\n",
    "\n",
    "Разберём на примере самый простой случай классификации, когда пространство признака одномерное, а нам нужно разделить 2 класса. Ситуация встречается чаще, чем может представиться: например, когда нужно отличить два сигнала, или сравнить паттерн с образцом. Пусть у нас есть обучающая выборка. При этом получается изображение, где по оси X будет мера похожести, а по оси Y -количество событий с такой мерой. Когда искомый объект похож на себя — получается левая гауссиана. Когда не похож — правая. Значение X=0.4 разделяет выборки так, что ошибочное решение минимизирует вероятность принятия любого неправильного решения. Именно поиском такого разделителя и является задача классификации.\n",
    "\n",
    "<img src=\"https://i.ibb.co/gdG3jGj/hist_comp.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Выравнивание цвета изображений \n",
    "\n",
    "Коррекция (Enhancement) предназначена для того, чтобы сделать изображение более подходящим, чем оригинал, для конкретного применения.\n",
    "Метод гамма-коррекции полезен, когда необходимо изменить контрастность и яркость изображения.\n",
    "Чтобы понять, что такое гамма-коррекция, сначала нужно разобраться в преобразовании Power Law Transformation. С входными изображениями f(x, y) и после преобразования T у нас есть увеличенное выходное изображение g(x, y): $g(x, y) = T[ f(x, y) ]$\n",
    "\n",
    "Если обозначить r, s как серый уровень f(x, y) и g(x, y) для любой точки (x, y), то формулу можно записать как:\n",
    "\n",
    "<img src=\"img/Chart-5.jpg\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Контраст изоражений\n",
    "\n",
    "Контраст изображения достигается за счет смещения верхних и нижних значений изображения, как показано на рисунке ниже.\n",
    "\n",
    "<img src=\"img/Chart-2.jpg\" alt=\"Drawing\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-22T11:48:03.509738Z",
     "start_time": "2021-02-22T11:48:03.229711Z"
    }
   },
   "outputs": [],
   "source": [
    "# Contrast stretching\n",
    "image = cv2.imread('img/sudoku.jpg')\n",
    "\n",
    "p_l = np.percentile(image, 2)\n",
    "p_h = np.percentile(image, 95)\n",
    "image_rescale = skimage.exposure.rescale_intensity(image, in_range=(p_l, p_h))\n",
    "\n",
    "plot_transform_result(image, image_rescale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist, bins = np.histogram(image.flatten(), 256, [0,256])\n",
    "hist_rescale, bins = np.histogram(image_rescale.flatten(), 256, [0,256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6.4 * 2, 4.8), constrained_layout=True)\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title('Исходная гистограмма')\n",
    "plt.plot(hist)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title('Результат преобразования')\n",
    "plt.plot(hist_rescale)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Гамма коррекция \n",
    "\n",
    "Для повышения яркости изображения мы изменим значение r, чтобы достичь значения s. Для выполнения работы определено преобразование: $s = c*r^y$\n",
    "\n",
    "Вышеуказанное преобразование использует r power γ (гамма), поэтому оно называется преобразованием Power Law Transformation. Изменяя значение γ, мы получаем разные результаты. Поэтому гамма-коррекция - это процесс выбора наилучшего значения, чтобы гамма имела наилучшее выходное изображение.\n",
    "\n",
    "<img src=\"img/Chart-6.jpg\" alt=\"Drawing\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-22T11:48:04.022711Z",
     "start_time": "2021-02-22T11:48:03.770710Z"
    }
   },
   "outputs": [],
   "source": [
    "# Gamma correction\n",
    "gamma_corrected = skimage.exposure.adjust_gamma(image, gamma=0.5)\n",
    "\n",
    "plot_transform_result(image, gamma_corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist, bins = np.histogram(image.flatten(), 256, [0,256])\n",
    "hist_rescale, bins = np.histogram(gamma_corrected.flatten(), 256, [0,256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6.4 * 2, 4.8), constrained_layout=True)\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title('Исходная гистограмма')\n",
    "plt.plot(hist)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title('Результат преобразования')\n",
    "plt.plot(hist_rescale)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Логарифмическая коррекция\n",
    "\n",
    "Преобразование по логарифму."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-22T11:48:04.502723Z",
     "start_time": "2021-02-22T11:48:04.246709Z"
    }
   },
   "outputs": [],
   "source": [
    "# Logarithmic\n",
    "logarithmic_corrected = skimage.exposure.adjust_log(image, gain=0.7)\n",
    "\n",
    "plot_transform_result(image, logarithmic_corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist, bins = np.histogram(image.flatten(), 256, [0,256])\n",
    "hist_rescale, bins = np.histogram(logarithmic_corrected.flatten(), 256, [0,256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6.4 * 2, 4.8), constrained_layout=True)\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title('Исходная гистограмма')\n",
    "plt.plot(hist)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title('Результат преобразования')\n",
    "plt.plot(hist_rescale)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLAHE\n",
    "\n",
    "Первое выравнивание гистограммы, которое мы только что видели, учитывает глобальный контраст изображения. Во многих случаях это не очень хорошая идея. Например, ниже изображение показывает входное изображение и его результат после глобальной выравнивания гистограммы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-22T11:48:05.123745Z",
     "start_time": "2021-02-22T11:48:04.717792Z"
    }
   },
   "outputs": [],
   "source": [
    "## вспомним, что было с линиаризацией освещенности\n",
    "image = cv2.imread('img/forest.jpg')\n",
    "gray = cv2.cvtColor(image.copy(), cv2.COLOR_BGR2GRAY)\n",
    "global_corrected_image = cv2.equalizeHist(gray)\n",
    "\n",
    "plot_transform_result(gray, global_corrected_image, is_gray=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Контрастность фона улучшилась после выравнивания гистограммы. Мы потеряли большую часть информации там из-за чрезмерной яркости. Это потому, что его гистограмма не ограничена определенной областью, как мы видели в предыдущих случаях.\n",
    "\n",
    "Поэтому для решения этой проблемы используется __адаптивное выравнивание гистограммы (AHE)__. При этом изображение делится на маленькие блоки, называемые «плитками» (tileSize по умолчанию в OpenCV составляет $8\\times8$). Затем каждый из этих блоков гистограммы выравнивается как обычно. Таким образом, в небольшой области гистограмма будет ограничена небольшой областью (если нет шума), как показано на рисунке:\n",
    "\n",
    "<img src=\"https://i.ibb.co/Z2v37jw/AHE-neighbourhoods.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "Если шум есть, он будет усилен. Чтобы избежать этого, применяется ограничение контраста. Если какой-либо интервал гистограммы превышает указанный предел контраста (по умолчанию $40$ в OpenCV), эти пиксели обрезаются и распределяются равномерно по другим интервалам перед применением выравнивания гистограммы. После выравнивания для удаления артефактов на границах мозаики применяется билинейная интерполяция.\n",
    "\n",
    "Ниже приведен фрагмент кода, демонстрирующий применение CLAHE в OpenCV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# применим CLAHE из cv2 и сравним результат с обычным варвниваем всей гистрограммы\n",
    "clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "clahe_image = clahe.apply(gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-22T11:48:06.176653Z",
     "start_time": "2021-02-22T11:48:05.124746Z"
    }
   },
   "outputs": [],
   "source": [
    "hist_gray = cv2.calcHist([gray], [0], None, [256], [0,256])\n",
    "hist_equ = cv2.calcHist([global_corrected_image], [0], None, [256], [0,256])\n",
    "hist_clahe = cv2.calcHist([clahe_image], [0], None, [256], [0,256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6.4 * 3, 4.8 * 1.7), constrained_layout=True)\n",
    "\n",
    "plt.subplot(231)\n",
    "plt.title('Исходное изображение')\n",
    "plt.imshow(gray, cmap='gray'), plt.axis('off')\n",
    "\n",
    "plt.subplot(232)\n",
    "plt.title('Результат глобальной коррекции гистограммы')\n",
    "plt.imshow(global_corrected_image, cmap='gray'), plt.axis('off')\n",
    "\n",
    "plt.subplot(233)\n",
    "plt.title('Результат коррекции с помощью CLAHE')\n",
    "plt.imshow(clahe_image, cmap='gray'), plt.axis('off')\n",
    "\n",
    "plt.subplot(234)\n",
    "plt.plot(hist_gray)\n",
    "\n",
    "plt.subplot(235)\n",
    "plt.plot(hist_equ)\n",
    "\n",
    "plt.subplot(236)\n",
    "plt.plot(hist_clahe)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Бинаризация изображений\n",
    "Здесь все просто. Если значение пикселя больше порогового значения, ему присваивается одно значение (может быть белым), в противном случае ему присваивается другое значение (может быть черным). Используемая функция **cv2.threshold(src, thresh, maxval, type)**\n",
    "\n",
    "* **src** $-$ исходное изображение, которое должно быть изображением в градациях серого\n",
    "* **thresh** $-$ пороговое значение, которое используется для классификации значений пикселей\n",
    "* **maxval** $-$ представляет значение, которое будет дано, если значение пикселя больше (иногда меньше) порогового значения\n",
    "* **type** $-$ предоставляет различные стили порогового значения.\n",
    "\n",
    "Различные типы:\n",
    "1. cv2.THRESH_BINARY\n",
    "2. cv2.THRESH_BINARY_INV\n",
    "3. cv2.THRESH_TRUNC\n",
    "4. cv2.THRESH_TOZERO\n",
    "5. cv2.THRESH_TOZERO_INV\n",
    "\n",
    "На выходе функция возвращает два значения. Первый $-$ **retval**, которое будет объяснено позже. Второй $-$ **thresholded image**.\n",
    "\n",
    "Подробнее в [документации](https://docs.opencv.org/3.4.2/d7/d1b/group__imgproc__misc.html#gae8a4a146d1ca78c626a53577199e9c57).\n",
    "\n",
    "Рассмотрим разные типы threshold на примере градиента серого цвета."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('img/grad_grayscale.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-22T11:48:12.071793Z",
     "start_time": "2021-02-22T11:48:10.952241Z"
    }
   },
   "outputs": [],
   "source": [
    "ret, thresh1 = cv2.threshold(img, 127, 255, cv2.TbHRESH_BINARY)\n",
    "ret, thresh2 = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY_INV)\n",
    "ret, thresh3 = cv2.threshold(img, 127, 255, cv2.THRESH_TRUNC)\n",
    "ret, thresh4 = cv2.threshold(img, 127, 255, cv2.THRESH_TOZERO)\n",
    "ret, thresh5 = cv2.threshold(img, 127, 255, cv2.THRESH_TOZERO_INV)\n",
    "\n",
    "\n",
    "titles = ['Original Image','BINARY','BINARY_INV','TRUNC','TOZERO','TOZERO_INV']\n",
    "images = [img, thresh1, thresh2, thresh3, thresh4, thresh5]\n",
    "plt.figure(figsize=(6.4, 4.8), constrained_layout=True)\n",
    "for i in range(6):\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    plt.imshow(images[i], cmap='gray')\n",
    "    plt.title(titles[i])\n",
    "    plt.axis('off')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive Thresholding\n",
    "В предыдущем разделе мы использовали глобальное значение в качестве порогового значения. Но это может быть не хорошо во всех условиях, когда изображение имеет разные условия освещения в разных областях. В этом случае мы идем на адаптивный порог. При этом алгоритм вычисляет порог для небольших областей изображения. Таким образом, мы получаем разные пороговые значения для разных областей одного и того же изображения, и это дает нам лучшие результаты для изображений с разной освещенностью.\n",
    "\n",
    "Он имеет три «специальных» входных параметра и только один выходной аргумент.\n",
    "\n",
    "Адаптивный метод $-$ **cv2.adaptiveThreshold(src, maxValue, adaptiveMethod, thresholdType, blockSize, C)** - решает, как рассчитывается пороговое значение.\n",
    "\n",
    "* **thresholdType**:\n",
    "    * *cv2.ADAPTIVE_THRESH_MEAN_C*: пороговое значение является средним значением области соседства.\n",
    "\n",
    "    * *cv2.ADAPTIVE_THRESH_GAUSSIAN_C*: пороговое значение представляет собой взвешенную сумму значений окрестностей, где веса представляют собой гауссово окно.\n",
    "\n",
    "* **blockSize** $-$ определяет размер окна.\n",
    "\n",
    "* **C** $-$ это константа, которая вычитается из вычисленного среднего или взвешенного среднего.\n",
    "\n",
    "Ниже приведен фрагмент кода, сравнивающий глобальные пороги и адаптивные пороги для изображения с различным освещением:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-22T11:48:12.326548Z",
     "start_time": "2021-02-22T11:48:12.072788Z"
    }
   },
   "outputs": [],
   "source": [
    "img = cv2.imread('img/sudoku.jpg')\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.medianBlur(img, 5)\n",
    "\n",
    "ret, th1 = cv2.threshold(img, 127, 255,cv2.THRESH_BINARY)\n",
    "th2 = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 11, 2)\n",
    "th3 = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = ['Original Image', 'Global Thresholding (v = 127)',\n",
    "            'Adaptive Mean Thresholding', 'Adaptive Gaussian Thresholding']\n",
    "images = [img, th1, th2, th3]\n",
    "plt.figure(figsize=(6.4 * 2, 4.8 * 2), constrained_layout=True)\n",
    "\n",
    "for i in range(4):\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    plt.imshow(images[i],'gray')\n",
    "    plt.title(titles[i])\n",
    "    plt.axis('off')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Бинаризация Оцу\n",
    "В первом разделе я говорил вам, что есть второй параметр retVal. Его использование происходит, когда мы идем на бинаризацию Оцу. Так что же это?\n",
    "\n",
    "В глобальном пороговом значении мы использовали произвольное значение для порогового значения, верно? Итак, как мы можем знать, что выбранное нами значение является хорошим или нет? Ответ, метод проб и ошибок. Но рассмотрим бимодальное изображение (проще говоря, бимодальное изображение - это изображение, гистограмма которого имеет два пика). Для этого изображения мы можем приблизительно принять значение в середине этих пиков в качестве порогового значения, верно? Это то, что делает бинаризация Оцу. Таким образом, простыми словами, он автоматически вычисляет пороговое значение из гистограммы изображения для бимодального изображения. (Для изображений, которые не являются бимодальными, бинаризация не будет точной.)\n",
    "\n",
    "Для этого используется наша функция **cv2.threshold()**, но передается дополнительный флаг *cv2.THRESH_OTSU*. Для порогового значения просто введите ноль. Затем алгоритм находит оптимальное пороговое значение и возвращает вас в качестве второго выхода retVal. Если пороговое значение Otsu не используется, **retVal** соответствует пороговому значению, которое вы использовали.\n",
    "\n",
    "Проверьте ниже пример. Входное изображение является шумным изображением. В первом случае я применил глобальный порог для значения $127$. Во втором случае я применил порог Оцу напрямую. В третьем случае я отфильтровал изображение с гауссовым ядром $5\\times5$, чтобы удалить шум, затем применил пороговое значение Оцу. Посмотрите, как фильтрация шума улучшает результат."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('img/otsu_ex.jpg')\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# внесем случайный шум\n",
    "noise_img = img.copy()\n",
    "mask = np.random.randint(0, 2, size=noise_img.shape).astype(bool)\n",
    "r = np.random.rand(*noise_img.shape) * np.max(noise_img)\n",
    "noise_img[mask] = r[mask]\n",
    "img = noise_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global thresholding\n",
    "ret1, th1 = cv2.threshold(img, 170, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "# Otsu's thresholding\n",
    "ret2, th2 = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "# Otsu's thresholding after Gaussian filtering\n",
    "blur = cv2.GaussianBlur(img, (5,5), 0)\n",
    "ret3, th3 = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-22T11:48:13.724676Z",
     "start_time": "2021-02-22T11:48:12.327549Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot all the images and their histograms\n",
    "images = [img, 0, th1,\n",
    "          img, 0, th2,\n",
    "          blur, 0, th3]\n",
    "ret = [ret1, ret2, ret3]\n",
    "titles = ['Original Noisy Image','Histogram','Global Thresholding (th=170)',\n",
    "          'Original Noisy Image','Histogram',\"Otsu's Thresholding\",\n",
    "          'Gaussian filtered Image','Histogram',\"Otsu's Thresholding\"]\n",
    "\n",
    "plt.figure(figsize=(6.4 * 2, 4.8 * 2), constrained_layout=True)\n",
    "\n",
    "for i in range(3):\n",
    "    plt.subplot(3, 3, i * 3 + 1), plt.imshow(images[i*3],'gray')\n",
    "    plt.title(titles[i * 3]), plt.axis('off')\n",
    "    \n",
    "    plt.subplot(3, 3, i * 3 + 2), plt.hist(images[i*3].ravel(), 256)\n",
    "    plt.title(titles[i * 3 + 1] + ', threshold = ' + str(ret[i]))\n",
    "    \n",
    "    plt.subplot(3, 3, i * 3 + 3), plt.imshow(images[i*3 + 2],'gray')\n",
    "    plt.title(titles[i * 3 + 2]), plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пример с цветокоррекцией\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('img/sudoku.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_msrcp = retinex.MSRCP(\n",
    "    img,\n",
    "    config['sigma_list'],\n",
    "    config['low_clip'],\n",
    "    config['high_clip']        \n",
    ")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) \n",
    "\n",
    "ret, th1 = cv2.threshold(img, 127, 255,cv2.THRESH_BINARY)\n",
    "th2 = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 11, 2)\n",
    "th3 = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-22T11:48:15.924057Z",
     "start_time": "2021-02-22T11:48:13.725647Z"
    }
   },
   "outputs": [],
   "source": [
    "titles = ['Original Image', 'Global Thresholding (v = 127)',\n",
    "            'Adaptive Mean Thresholding', 'Adaptive Gaussian Thresholding']\n",
    "images = [img_msrcp, th1, th2, th3]\n",
    "plt.figure(figsize=(6.4 * 2, 4.8 * 2), constrained_layout=True)\n",
    "\n",
    "for i in range(4):\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    plt.imshow(images[i],'gray')\n",
    "    plt.title(titles[i]), plt.axis('off')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Морфологические трансформации\n",
    "Морфологические преобразования $-$ это простые операции, основанные на форме изображения. Обычно это выполняется на двоичных изображениях. Он требует двух входных данных, один из которых является нашим исходным изображением, второй называется структурирующим элементом или ядром, которое определяет характер операции. Два основных морфологических оператора - это **эрозия** и **растягивание**. Затем его разновидности, такие как открытие, закрытие, градиент и т. д.\n",
    "\n",
    "**Эрозия** (размывание/сужение) изображения обычно используется для избавления от случайных вкраплений на изображении. Идея состоит в том, что вкрапления при размывании устранятся, тогда как крупные и соответсвенно более визуально-значимые регионы остаются.\n",
    "\n",
    "**Растягивание** (расширение) же, по идее, так же должно устранять шум и способствовать объединению областей изображения, которые были разделены шумом, тенями.\n",
    "Применение же небольшого растягивания должно сплавить эти области в одну.\n",
    "\n",
    "Морфологические операции, чаще всего, применяются над двоичными изображениями, которые получаются после порогового преобразования (thresholding).\n",
    "***\n",
    "**Подробнее с математической морфологией можно ознакомиться по этим ссылка: [1](https://ru.wikipedia.org/wiki/%CC%E0%F2%E5%EC%E0%F2%E8%F7%E5%F1%EA%E0%FF_%EC%EE%F0%F4%EE%EB%EE%E3%E8%FF), [2](https://habr.com/post/113626/), [3](http://wiki.technicalvision.ru/index.php/%D0%9C%D0%BE%D1%80%D1%84%D0%BE%D0%BB%D0%BE%D0%B3%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D0%B5_%D0%BE%D0%BF%D0%B5%D1%80%D0%B0%D1%86%D0%B8%D0%B8_%D0%BD%D0%B0_%D0%B1%D0%B8%D0%BD%D0%B0%D1%80%D0%BD%D1%8B%D1%85_%D0%B8%D0%B7%D0%BE%D0%B1%D1%80%D0%B0%D0%B6%D0%B5%D0%BD%D0%B8%D1%8F%D1%85).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Эрозия\n",
    "\n",
    "Основная идея эрозии похожа только на эрозию почвы, она размывает границы объекта переднего плана (всегда старайтесь, чтобы передний план оставался белым). Так, что это делает? Ядро скользит по изображению (как в **2D**-свертке). Пиксель в исходном изображении ($1$ или $0$) будет считаться $1$, только если все пиксели под ядром равны $1$, в противном случае он размыт (обнуляется).\n",
    "\n",
    "Итак, что происходит, так это то, что все пиксели вблизи границы будут отбрасываться в зависимости от размера ядра. Таким образом, толщина или размер объекта переднего плана уменьшается или просто белая область уменьшается на изображении. Это полезно для удаления небольших белых шумов (как мы видели в главе о цветовом пространстве), отсоединения двух связанных объектов и т. д.\n",
    "\n",
    "Здесь, в качестве примера, я бы использовал полное ядро $5\\times5$. Давайте посмотрим, как это работает:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread('img/j.png')\n",
    "\n",
    "print(f'Размер изображения: {image.shape}')\n",
    "\n",
    "kernel = np.ones((5, 5), np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-22T13:43:20.143185Z",
     "start_time": "2021-02-22T13:43:19.907876Z"
    }
   },
   "outputs": [],
   "source": [
    "erosion = cv2.erode(image, kernel, iterations=1)\n",
    "\n",
    "plot_transform_result(image, erosion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Растягивание\n",
    "\n",
    "Это просто противоположность эрозии. Здесь пиксельный элемент равен «$1$», если хотя бы один пиксель под ядром равен «$1$». Таким образом, увеличивается белая область в изображении или увеличивается размер объекта переднего плана. Обычно в таких случаях, как удаление шума, за эрозией следует расширение. Потому что эрозия удаляет белые шумы, но также уменьшает наш объект. Таким образом, мы расширяем это. Поскольку шум исчез, они не вернутся, но наша площадь объекта увеличивается. Это также полезно при соединении сломанных частей объекта."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-22T13:43:21.227640Z",
     "start_time": "2021-02-22T13:43:20.990439Z"
    }
   },
   "outputs": [],
   "source": [
    "dilation = cv2.dilate(image, kernel, iterations=3)\n",
    "\n",
    "plot_transform_result(image, dilation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Открытие\n",
    "\n",
    "Открытие - это  последовательное применение эрозии и закрытия. Это полезно для удаления шума, как мы объяснили выше. Здесь мы используем функцию **cv2.morphologyEx()**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# внесем шум\n",
    "noise_img = image.copy()\n",
    "mask = np.random.randint(0, 2, size=noise_img.shape).astype(bool)\n",
    "r = np.ones(noise_img.shape) * 255\n",
    "noise_img[mask] = r[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-22T13:43:21.942527Z",
     "start_time": "2021-02-22T13:43:21.607589Z"
    }
   },
   "outputs": [],
   "source": [
    "opening = cv2.morphologyEx(noise_img, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "plot_transform_result(noise_img, opening)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Закрытие\n",
    "\n",
    "Закрытие является обратной процедурой к открытию, растягивание с последующей эрозией. Это полезно при закрытии небольших отверстий внутри объектов переднего плана или маленьких черных точек на объекте."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# внесем шум\n",
    "noise_img = image.copy()\n",
    "mask = np.random.randint(0, 2, size=noise_img.shape).astype(bool)\n",
    "r = np.zeros(noise_img.shape)\n",
    "noise_img[mask] = r[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-22T13:43:26.218305Z",
     "start_time": "2021-02-22T13:43:25.993657Z"
    }
   },
   "outputs": [],
   "source": [
    "closing = cv2.morphologyEx(noise_img, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "plot_transform_result(noise_img, closing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Морфологический градиент\n",
    "\n",
    "Это разница между эрозией и растягиванием изображения. Результат будет выглядеть как контур объекта."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-22T13:47:39.461528Z",
     "start_time": "2021-02-22T13:47:39.251264Z"
    }
   },
   "outputs": [],
   "source": [
    "erode = cv2.erode(image, kernel, iterations=1)\n",
    "dilate = cv2.dilate(image, kernel, iterations=1)\n",
    "gradient = dilate - erode\n",
    "\n",
    "plot_transform_result(image, gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-22T11:48:16.771759Z",
     "start_time": "2021-02-22T11:48:16.637688Z"
    }
   },
   "outputs": [],
   "source": [
    "gradient = cv2.morphologyEx(image, cv2.MORPH_GRADIENT, kernel)\n",
    "\n",
    "plot_transform_result(image, gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Пример\n",
    "\n",
    "Рассмотрим предыдущий пример с бинаризацией изображения методом Оцу. Получили что-то такое."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread('img/otsu_ex.jpg')\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# внесем случайный шум\n",
    "noise_img = image.copy()\n",
    "mask = np.random.randint(0, 2, size=noise_img.shape).astype(bool)\n",
    "r = np.random.rand(*noise_img.shape) * np.max(noise_img)\n",
    "noise_img[mask] = r[mask]\n",
    "image = noise_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-22T11:48:16.915755Z",
     "start_time": "2021-02-22T11:48:16.772729Z"
    }
   },
   "outputs": [],
   "source": [
    "blur = cv2.GaussianBlur(image, (5,5), 0)\n",
    "ret3, th3 = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "plot_transform_result(image, th3, is_gray=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что остались шумовые точки. Попробуем избавиться от них и получить целостную маску без артефактов с помощью морфологических операций."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-22T11:48:17.030727Z",
     "start_time": "2021-02-22T11:48:16.916750Z"
    }
   },
   "outputs": [],
   "source": [
    "opening = cv2.morphologyEx(th3, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "plot_transform_result(th3, opening, is_gray=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-22T11:48:17.146730Z",
     "start_time": "2021-02-22T11:48:17.031726Z"
    }
   },
   "outputs": [],
   "source": [
    "closing = cv2.morphologyEx(opening, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "plot_transform_result(th3, closing, is_gray=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-22T11:48:17.262728Z",
     "start_time": "2021-02-22T11:48:17.148729Z"
    }
   },
   "outputs": [],
   "source": [
    "kernel = np.array([\n",
    "    [0, 0, 1, 0, 0],\n",
    "    [0, 1, 1, 1, 0],\n",
    "    [1, 1, 1, 1, 1],\n",
    "    [0, 1, 1, 1, 0],\n",
    "    [0, 0, 1, 0, 0],\n",
    "], np.uint8)\n",
    "dilation = cv2.dilate(closing, kernel, iterations=5)\n",
    "\n",
    "plot_transform_result(closing, dilation, is_gray=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Пирамиды изображений\n",
    "\n",
    "Обычно мы работали с изображением постоянного размера. Но в некоторых случаях нам нужно работать с изображениями разного разрешения одного и того же изображения. Например, при поиске чего-либо на изображении, например лица, мы не уверены, в каком размере будет присутствовать объект на изображении. В этом случае нам нужно будет создать набор изображений с разным разрешением и выполнить поиск объекта по всем изображениям. Эти наборы изображений с разным разрешением называются пирамидами изображений (потому что, когда они хранятся в стопке с самым большим изображением внизу и самым маленьким изображением сверху, они выглядят как пирамида).\n",
    "\n",
    "Существует два вида пирамид изображений:\n",
    "1. Пирамида Гаусса\n",
    "2. Пирамиды Лапласа\n",
    "\n",
    "Более высокий уровень (низкое разрешение) в гауссовой пирамиде формируется путем удаления последовательных строк и столбцов в изображении нижнего уровня (более высокое разрешение). Затем каждый пиксель на более высоком уровне формируется вкладом из 5 пикселей на базовом уровне с гауссовыми весами. Таким образом, изображение $M \\times N$ становится изображением $M / 2 \\times N / 2$. То есть, площадь уменьшается до четверти первоначальной площади. Это называется Октава. Та же самая картина продолжается, когда мы идем вверх в пирамиде (то есть разрешение уменьшается). Аналогично, при расширении область становится 4 раза на каждом уровне. Мы можем найти гауссовы пирамиды, используя функции ```cv2.pyrDown()``` и ```cv2.pyrUp()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread('img/forest.jpg')\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-22T11:48:17.793805Z",
     "start_time": "2021-02-22T11:48:17.263730Z"
    }
   },
   "outputs": [],
   "source": [
    "h, w, _ = image.shape\n",
    "# создадим пустую подложку\n",
    "zero_back = np.zeros((h, w, 3), np.uint8)\n",
    "\n",
    "# сделаем элемент пирамиды (уменьшим)\n",
    "lower_reso = cv2.pyrDown(image)\n",
    "h, w, _ = lower_reso.shape\n",
    "zero_back[:h, :w, :] = lower_reso  # разместим преобразованное изображение на подложку\n",
    "\n",
    "# отобразим результат\n",
    "plot_transform_result(image, zero_back)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь вы можете перейти вниз по пирамиде изображения с помощью функции ```cv2.pyrUp()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-22T11:48:18.304620Z",
     "start_time": "2021-02-22T11:48:17.794771Z"
    }
   },
   "outputs": [],
   "source": [
    "# сделаем элемент пирамиды (увеличим) уже уменьшенное\n",
    "higher_reso = cv2.pyrUp(lower_reso)\n",
    "\n",
    "# отобразим результат\n",
    "plot_transform_result(image, higher_reso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(image - higher_reso) / (image.shape[0] * image.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лапласовские пирамиды образованы из гауссовых пирамид. Для этого нет специальной функции. Изображения пирамиды Лапласа подобны только краевым изображениям. Большинство его элементов - нули. Они используются в сжатии изображений. Уровень в лапласовой пирамиде формируется разницей между этим текущим в гауссовой пирамиде и ее расширенной версией верхнего уровня в гауссовой пирамиде."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Генерируем пирамиды Гаусса\n",
    "G = image.copy()\n",
    "gpA = [G]\n",
    "for i in range(6):\n",
    "    G = cv2.pyrDown(G)\n",
    "    gpA.append(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-22T11:48:18.342626Z",
     "start_time": "2021-02-22T11:48:18.305594Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Генерируем пирамиды Лапласа\n",
    "lpA = [gpA[5]]\n",
    "\n",
    "for i in range(5,0,-1):\n",
    "    GE = cv2.pyrUp(gpA[i])\n",
    "    h_ge, w_ge, _ = GE.shape\n",
    "    h_a, w_a, _ = gpA[i-1].shape\n",
    "    print(f'Разница между высотой и шириной:', abs(h_ge-h_a), abs(w_ge-w_a))\n",
    "    \n",
    "    # делаем падинг для согласования размеров изображений\n",
    "    gpA[i-1] = np.pad(gpA[i-1], pad_width=[(0, abs(h_ge-h_a)), (0, abs(w_ge-w_a)), (0, 0)], mode='edge')\n",
    "    print(f'Размеры изображений после падинга: {GE.shape, gpA[i-1].shape}\\n')\n",
    "    \n",
    "    L = cv2.subtract(gpA[i-1], GE)\n",
    "    lpA.append(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-22T11:48:18.972641Z",
     "start_time": "2021-02-22T11:48:18.343625Z"
    }
   },
   "outputs": [],
   "source": [
    "# Визуализация пирамид\n",
    "plt.figure(figsize=(6.4 * 2, 4.8 * 1), constrained_layout=False)\n",
    "\n",
    "for i in range(len(lpA)):\n",
    "    tmp = lpA[i].copy()\n",
    "    if i > 1:\n",
    "        tmp *= 10  # повысим контраст, чтобы было видно\n",
    "        \n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    plt.imshow(tmp)\n",
    "    plt.title(f'Уровень пирамиды: {i + 1}')\n",
    "    plt.axis('off')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blending\n",
    "\n",
    "Одним из применений пирамид является **смешивание изображений (blending)**. Например, при сшивании изображений вам нужно будет сложить два изображения вместе, но это может выглядеть не очень хорошо из-за разрыва между изображениями. В этом случае смешивание изображений с пирамидами обеспечивает плавное смешивание, не оставляя большого количества данных на изображениях. Одним из классических примеров этого является смешивание двух фруктов, апельсина и яблока. Теперь посмотрите на результат:\n",
    "\n",
    "<img src=\"img/orapple.jpg\" alt=\"Drawing\" style=\"width: 350px;\"/>\n",
    "\n",
    "Делается следующим образом:\n",
    "\n",
    "* Загрузите два изображения яблока и апельсина\n",
    "* Найдите гауссовы пирамиды для яблок и апельсинов (в данном конкретном примере количество уровней равно 6)\n",
    "* Из гауссовых пирамид, найдите их лапласианские пирамиды\n",
    "* Теперь соедините левую половину яблока и правую половину апельсина на каждом уровне лапласианских пирамид\n",
    "* Наконец из этого совместного изображения пирамид, восстановить исходное изображение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Практика\n",
    "\n",
    "Смешать два изображения (blending) с помощью пирамид. Примеры изображений загружены ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-22T11:48:19.222165Z",
     "start_time": "2021-02-22T11:48:18.973640Z"
    }
   },
   "outputs": [],
   "source": [
    "img_left = cv2.imread('img/cap_1.jpg')\n",
    "img_left = cv2.cvtColor(img_left, cv2.COLOR_BGR2RGB)\n",
    "img_right = cv2.imread('img/cap_2.jpg')\n",
    "img_right = cv2.cvtColor(img_right, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "print(f'Размер 1 изображения: {img_left.shape} \\nРазмер 2 изображения: {img_right.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6.4 * 2, 4.8 * 1), constrained_layout=False)\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title('Изображение раз')\n",
    "plt.imshow(img_left)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title('Изображение два')\n",
    "plt.imshow(img_right)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-22T11:48:19.232354Z",
     "start_time": "2021-02-22T11:48:19.224164Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_gp_lp(image, rg=6):\n",
    "    # Генерируем пирамиды Гаусса\n",
    "    G = image.copy()\n",
    "    gpA = [G]\n",
    "    for i in range(rg):\n",
    "        G = cv2.pyrDown(G)\n",
    "        gpA.append(G)\n",
    "\n",
    "    # Генерируем пирамиды Лапласа\n",
    "    lpA = [gpA[rg - 1]]\n",
    "    for i in range(rg - 1, 0, -1):\n",
    "        GE = cv2.pyrUp(gpA[i])\n",
    "        h_ge, w_ge, _ = GE.shape\n",
    "        h_a, w_a, _ = gpA[i-1].shape\n",
    "\n",
    "        # делаем падинг для согласования размеров изображений\n",
    "        gpA[i-1] = np.pad(gpA[i-1], pad_width=[(0, abs(h_ge-h_a)), (0, abs(w_ge-w_a)), (0, 0)], mode='edge')\n",
    "\n",
    "        L = cv2.subtract(gpA[i-1], GE)\n",
    "        lpA.append(L)\n",
    "        \n",
    "    return gpA, lpA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-22T11:48:19.252194Z",
     "start_time": "2021-02-22T11:48:19.233164Z"
    }
   },
   "outputs": [],
   "source": [
    "rg = 5\n",
    "gpA, lpA = create_gp_lp(img_left, rg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-22T11:48:19.262188Z",
     "start_time": "2021-02-22T11:48:19.254191Z"
    }
   },
   "outputs": [],
   "source": [
    "gpB, lpB = create_gp_lp(img_right, rg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-22T11:48:19.269191Z",
     "start_time": "2021-02-22T11:48:19.264190Z"
    }
   },
   "outputs": [],
   "source": [
    "# конкатинируем пары слева и справа на каждом уровне пирамид\n",
    "LS = []\n",
    "for la, lb in zip(lpA, lpB):\n",
    "    rows, cols, dpt = la.shape\n",
    "    ls = np.hstack((la[:, 0:cols//2], lb[:, cols//2:]))\n",
    "    LS.append(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-22T11:48:19.648255Z",
     "start_time": "2021-02-22T11:48:19.349193Z"
    }
   },
   "outputs": [],
   "source": [
    "# соединияем уровни\n",
    "ls_ = LS[0]\n",
    "for i in range(1, rg):\n",
    "    ls_ = cv2.pyrUp(ls_)\n",
    "    ls_ = cv2.add(ls_, LS[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# соеденим части изображений\n",
    "real = np.hstack((ls_[:, :cols//2], ls_[:, cols//2:]))\n",
    "\n",
    "combined_image = np.hstack((img_left, img_right))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6.4 * 2, 4.8 * 1), constrained_layout=True)\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title('Результат блендинга')\n",
    "plt.imshow(real)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title('Исходные пары')\n",
    "plt.imshow(combined_image)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "289px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "145px",
    "left": "1416.99px",
    "right": "20px",
    "top": "125.992px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
